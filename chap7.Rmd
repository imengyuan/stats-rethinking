---
title: "Bayesian Chap 7 Assignment"
author: "Meng Yuan" 
output: pdf_document
fontsize: 12pt 
---

## Question 1

Information entropy is defined as $H(p)=-\sum_{i=1}^{n} p_{i} \log \left(p_{i}\right)$, where $p_{i}$ is the probability of the $i$th event, the probabilities of n events sum to 1. Here's the function to compute H(p) in R:

```{r}
H <- function(p) -sum(p*log(p))
```

Here are the information entropies of three islands:
```{r}
island <- list()
island[[1]] <- c( 0.2 , 0.2 , 0.2 , 0.2 , 0.2 )
island[[2]] <- c( 0.8 , 0.1 , 0.05 , 0.025 , 0.025 )
island[[3]] <- c( 0.05 , 0.15 , 0.7 , 0.05 , 0.05 )
sapply( island , H )
```

Island 1's birb distribution has the largest entropy, island 2 has the smallest. Information entropy measures the uncertainty of a distribution, the more even the distribution is, the more uncertainty it has, the larger entropy it has.
Island 1 has the most even distribution, each probability of the birb is not surprising, hence it has the largest uncertainty. In contrast, island 2 has the most uneven distribution, it has the least uncertainty and smallest entropy.

Next, we can calculate the K-L divergence of each island from others, to see which island's birb distribution best predicts the other two. K-L divergence is the amount of additional uncertainty added by using probabilities from one distribution to describe another distribution.

Here's the function to compute K-L divergence in R:
```{r}
DKL <- function(p,q) sum( p*(log(p)-log(q)) ) # distance from pq to p
```

Here are K-L distances in different ordered pairings:
```{r}
D <- matrix( NA , nrow=3 , ncol=3 )
for ( i in 1:3 ) for ( j in 1:3 ) D[i,j] <- DKL( island[[j]] , island[[i]])
D
```

Each row is a model, and each column is a true distribution. Each value represents the K-L distance from the model to the true distribution. The K-L distance between a model and itself is zero. Island 1 has the smallest distances to other islands, as the first row shows. Island 1 best predicts the other two islands, because it has the largest entropy, and it's less surprised by the probabilities of other islands.


\  

## Question 2

```{r include=FALSE}
library(rethinking)
d <- sim_happiness( seed=1977 , N_years=1000 )
precis(d)
d2 <- d[ d$age>17 , ] # only adults
d2$A <- ( d2$age - 18 ) / ( 65 - 18 )

```

Here are model m6.9 and m6.10:

```{r}
# m6.9
d2$mid <- d2$married + 1
m6.9 <- quap(
    alist(
        happiness ~ dnorm( mu , sigma ),
        mu <- a[mid] + bA*A,
        a[mid] ~ dnorm( 0 , 1 ),
        bA ~ dnorm( 0 , 2 ),
        sigma ~ dexp(1)
    ) , data=d2 )
precis(m6.9,depth=2)
```

```{r}
# m6.10
m6.10 <- quap(
    alist(
        happiness ~ dnorm( mu , sigma ),
        mu <- a + bA*A,
        a ~ dnorm( 0 , 1 ),
        bA ~ dnorm( 0 , 2 ),
        sigma ~ dexp(1)
    ) , data=d2 )
precis(m6.10)
```

Model m6.9 includes age and an index variable marriage status, while model m6.10 only includes age as predictor. Model m6.9 suggests that age is negatively associated with happiness, while model m6.10 suggests no association between age and happiness.

In model m6.9, marriage status is a collider that was conditioned on, it leads to spurious statistical association among the causes (age and marriage status) as well as erroneous causal inference.

Next, we can look at the predictive accuracies of both models through WAIC:

```{r}
compare( m6.9 , m6.10 , func=WAIC )
```
WAIC is an information criterion which approximates the out-of-sample K-L Divergence. It can be used to compare model predictive accuracies. Model 6.9 has better predictive accuracy (100% of weight) as the collider shows actual association. The model provides erroneous causal inference but has better prediction. This suggests causal inference and predictive accuracy should be considered separately, WAIC (or LOO) should not be used for causal inference. 

\  

## Question 3

I plan to use three sets of priors for the intercept $\alpha$ and slope $\beta$:

1. Regularized priors: $\alpha \sim Normal(0, 0.2), \beta \sim Normal(0, 0.3)$

2. Weak priors: $\alpha \sim Normal(0, 1), \beta \sim Normal(0, 1)$

3. Strong priors: $\alpha \sim Normal(0, 0.1), \beta \sim Normal(0, 0.1)$

```{r include=FALSE}
library(rethinking)
data(foxes)
d <- foxes
d$W <- standardize(d$weight)
d$A <- standardize(d$area)
d$F <- standardize(d$avgfood)
d$G <- standardize(d$groupsize)
```

Here are the codes for models using `quap` with regularized priors for the slopes and intercepts, for example:

```{r}
# F, G, A -> W
m1 <- quap(
    alist(
        W ~ dnorm( mu , sigma ),
        mu <- a + bF*F + bG*G + bA*A,
        a ~ dnorm(0,0.2),
        c(bF,bG,bA) ~ dnorm(0,0.5),
        sigma ~ dexp(1)
    ), data=d )
# F, G -> W 
m2 <- quap(
    alist(
        W ~ dnorm( mu , sigma ),
        mu <- a + bF*F + bG*G,
        a ~ dnorm(0,0.2),
        c(bF,bG) ~ dnorm(0,0.5),
        sigma ~ dexp(1)
    ), data=d )
# G, A -> W
m3 <- quap(
    alist(
        W ~ dnorm( mu , sigma ),
        mu <- a + bG*G + bA*A,
        a ~ dnorm(0,0.2),
        c(bG,bA) ~ dnorm(0,0.5),
        sigma ~ dexp(1)
    ), data=d )
# F -> W
m4 <- quap(
    alist(
        W ~ dnorm( mu , sigma ),
        mu <- a + bF*F,
        a ~ dnorm(0,0.2),
        bF ~ dnorm(0,0.5),
        sigma ~ dexp(1)
    ), data=d )
# A -> W
m5 <- quap(
    alist(
        W ~ dnorm( mu , sigma ),
        mu <- a + bA*A,
        a ~ dnorm(0,0.2),
        bA ~ dnorm(0,0.5),
        sigma ~ dexp(1)
), data=d )
```


### Model comparison

Here's model comparison using WAIC:

```{r}
compare(m1, m2, m3, m4, m5)
```

Model m1, m2 and m3 are the top three models, with smaller WAIC scores than the rest, and they share nearly all of the weight. But the differences in WAIC are small for all the models, compared to the standard error. With regard to WAIC, the top three models are similar, they all have groupsize as a predictor. This could mean that when groupsize is included as a predictor, the inclusion of avgfood or area as predictors doesn't affect the inference. According to the DAG, the influence of area on weight in through avgfood and groupsize, this also explains why the first three models are tied together.

When groupsize is not included as a predictor, model m4 and m5 are tied. area and avgfood have no back door and they both have no influence on weight, according to the posterior predictions. 

```{r}
precis(m4)
precis(m5)
```

\  

### Prior choice

Using model m1 as an example, here's the comparison between different priors.

```{r}
# weak priors
m1.1 <- quap(
    alist(
        W ~ dnorm( mu , sigma ),
        mu <- a + bF*F + bG*G + bA*A,
        a ~ dnorm(0,1),
        c(bF,bG,bA) ~ dnorm(0,1),
        sigma ~ dexp(1)
    ), data=d )

# strong priors
m1.2 <- quap(
    alist(
        W ~ dnorm( mu , sigma ),
        mu <- a + bF*F + bG*G + bA*A,
        a ~ dnorm(0,0.1),
        c(bF,bG,bA) ~ dnorm(0,0.1),
        sigma ~ dexp(1)
    ), data=d )
```

Based on the posterior predictive checks, regularized and weak priors provide better prediction than the strong prior. Though three models have poor predictions on some extreme values. 

```{r echo=FALSE}
par(mfrow=c(1,2))
mu <- link( m1 )
mu_mean <- apply( mu , 2 , mean )
mu_PI <- apply( mu , 2 , PI )
N_sim <- sim( m1 , n=1e3 )
N_PI <- apply( N_sim , 2 , PI )
plot( mu_mean ~ d$W , col=rangi2 , ylim=range(mu_PI),
      xlab="Observed weight (std)" , ylab="Predicted weight (std)" )
abline( a=0 , b=1, lty=2 )
mtext("Regularized prior")
for ( i in 1:nrow(d) ) lines( rep(d$W[i],2) , mu_PI[,i] , col=rangi2 )

mu <- link( m1.1 )
mu_mean <- apply( mu , 2 , mean )
mu_PI <- apply( mu , 2 , PI )
N_sim <- sim( m1.1 , n=1e3 )
N_PI <- apply( N_sim , 2 , PI )
plot( mu_mean ~ d$W , col=rangi2 , ylim=range(mu_PI) ,
      xlab="Observed weight (std)" , ylab="Predicted weight (std)" )
abline( a=0 , b=1 , lty=2 )
mtext("Weak prior")
for ( i in 1:nrow(d) ) lines( rep(d$W[i],2) , mu_PI[,i] , col=rangi2 )
```

```{r echo=FALSE}
par(mfrow=c(1,2))
mu <- link( m1.2 )
mu_mean <- apply( mu , 2 , mean )
mu_PI <- apply( mu , 2 , PI )
N_sim <- sim( m1.2 , n=1e3 )
N_PI <- apply( N_sim , 2 , PI )
plot( mu_mean ~ d$W , col=rangi2 , ylim=range(mu_PI) ,
      xlab="Observed weight (std)" , ylab="Predicted weight (std)" )
abline( a=0 , b=1 , lty=2 )
mtext("Strong prior")
for ( i in 1:nrow(d) ) lines( rep(d$W[i],2) , mu_PI[,i] , col=rangi2 )
```


WAIC supports that models m1 (regularized prior) and m1.1 (weak prior) are better than model m1.2 (strong prior). The model using regularized prior has the best prediction.

```{r}
compare(m1 , m1.1 , m1.2)
```

Model m1 and m1.1 also have close posterior means, with contrast to model m1.2.

```{r}
coeftab(m1 , m1.1 , m1.2)
```

These results show that regularized priors can let the model learn from the data while reducing the risk of overfitting, and provide better prediction. In this case, model m1 uses regularized priors and has the best prediction. Model m1.1 using vague priors does not work a lot worse, meaning there're no issues with overfitting. Model m1.2 using strong priors suffers from underfitting and provides the least accurate prediction.



